# GSP343 Optimize Costs for Google Kubernetes Engine: Challenge Lab

Topics tested:

- Deploying an app on a multi-tenant cluster
- Migrating cluster workloads to an optimized node pool
- Rolling out an application update while maintaining cluster availability
- Cluster and pod autoscaling


## Task 1: Create our cluster and deploy our app

- Create the cluster in the `us-central1` region
- The naming scheme is team-resource, e.g. a cluster could be named `onlineboutique-cluster`
- For your initial cluster, start with machine size `n1-standard-2` (2 vCPU, 8G memory)
- Start small and make a zonal cluster with only two (2) nodes.


```bash
ZONE=us-central1-a
gcloud container clusters create onlineboutique-cluster \
    --project=${DEVSHELL_PROJECT_ID} --zone=${ZONE} \
    --machine-type=n1-standard-2 --num-nodes=2

kubectl create namespace dev
kubectl create namespace prod

kubectl config set-context --current --namespace dev

git clone https://github.com/GoogleCloudPlatform/microservices-demo.git &&
cd microservices-demo && kubectl apply -f ./release/kubernetes-manifests.yaml --namespace dev

```

## Task 2: Migrate to an Optimized Nodepool

Hints: https://www.qwiklabs.com/focuses/15577?parent=catalog

Create a new node pool named `optimized-pool` with `custom-2-3584` as the machine type. Set the number of nodes to 2.

Once the new nodepool is set up, migrate your application's deployments to the new nodepool by cordoning off and draining default-pool. Delete the default-pool once the deployments have safely migrated.

```bash
gcloud container node-pools create optimized-pool \
  --cluster=onlineboutique-cluster \
  --machine-type=custom-2-3584 \
  --num-nodes=2 \
  --zone=$ZONE

for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
  kubectl cordon "$node";
done

for node in $(kubectl get nodes -l cloud.google.com/gke-nodepool=default-pool -o=name); do
  kubectl drain --force --ignore-daemonsets --delete-local-data --grace-period=10 "$node";
done

kubectl get pods -o=wide

```

With the pods migrated, it's safe to delete the old node pool:


```bash
gcloud container node-pools delete default-pool --cluster onlineboutique-cluster --zone $ZONE

```


## Task 3: Apply a Frontend Update

Set a pod disruption budget for your `frontend` deployment. Name it `onlineboutique-frontend-pdb` and set the min-availability of your deployment to 1.

```bash
kubectl create poddisruptionbudget onlineboutique-frontend-pdb --selector app=frontend --min-available 1  --namespace dev

KUBE_EDITOR="nano" kubectl edit kubectl edit deployment/frontend --namespace dev

```

change **image** to `gcr.io/qwiklabs-resources/onlineboutique-frontend:v2.1`, and
**imagePullPolicy** to `Always`


## Task 4: Autoscale from Estimated Traffic

Apply **horizontal pod autoscaling** to your frontend deployment in order to handle the traffic surge. Scale based off a target cpu percentage of 50 and set the pod scaling between 1 minimum and 13 maximum.


```bash
kubectl autoscale deployment frontend --cpu-percent=50 --min=1 --max=13
kubectl get hpa

```

To make sure the scaling action occurs without downtime, set the deployment to scale with a target cpu percentage of 50%. This should allow plenty of space to handle the load as the autoscaling occurs. Set the deployment to scale between 1 minimum and 10 maximum pods.

Update your **cluster autoscaler** to scale between 1 node minimum and 6 nodes maximum.

```bash
gcloud beta container clusters update onlineboutique-cluster --enable-autoscaling --min-nodes 1 --max-nodes 6 --zone $ZONE

```


```bash
kubectl exec $(kubectl get pod --namespace=dev | grep 'loadgenerator' | cut -f1 -d ' ') -it --namespace=dev -- bash -c "export USERS=8000; sh ./loadgen.sh"

```